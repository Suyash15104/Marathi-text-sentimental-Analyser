{
  "best_global_step": 2400,
  "best_metric": 0.6374692916870117,
  "best_model_checkpoint": "./results\\checkpoint-2400",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.3565163314342499,
      "learning_rate": 4.986111111111111e-05,
      "loss": 1.0992,
      "step": 10
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.1535828858613968,
      "learning_rate": 4.972222222222223e-05,
      "loss": 1.0972,
      "step": 20
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.24962827563285828,
      "learning_rate": 4.958333333333334e-05,
      "loss": 1.0978,
      "step": 30
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 3.6871895790100098,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 1.0855,
      "step": 40
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 1.4803669452667236,
      "learning_rate": 4.930555555555556e-05,
      "loss": 1.0836,
      "step": 50
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.4202674627304077,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 1.087,
      "step": 60
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.4137102961540222,
      "learning_rate": 4.902777777777778e-05,
      "loss": 1.1093,
      "step": 70
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.5856361985206604,
      "learning_rate": 4.888888888888889e-05,
      "loss": 1.1025,
      "step": 80
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.9125446677207947,
      "learning_rate": 4.875e-05,
      "loss": 1.0947,
      "step": 90
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.5131767988204956,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 1.0689,
      "step": 100
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 1.2225394248962402,
      "learning_rate": 4.8472222222222224e-05,
      "loss": 1.0652,
      "step": 110
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5598727464675903,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 1.0651,
      "step": 120
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.6257722973823547,
      "learning_rate": 4.819444444444445e-05,
      "loss": 1.0691,
      "step": 130
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.8871465921401978,
      "learning_rate": 4.805555555555556e-05,
      "loss": 1.04,
      "step": 140
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.8736250996589661,
      "learning_rate": 4.791666666666667e-05,
      "loss": 1.0014,
      "step": 150
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.1671977043151855,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 1.0647,
      "step": 160
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 18.442920684814453,
      "learning_rate": 4.7638888888888887e-05,
      "loss": 0.9707,
      "step": 170
    },
    {
      "epoch": 0.15,
      "grad_norm": 7.552261829376221,
      "learning_rate": 4.75e-05,
      "loss": 1.0436,
      "step": 180
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 1.3080493211746216,
      "learning_rate": 4.736111111111111e-05,
      "loss": 0.9747,
      "step": 190
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.5081389546394348,
      "learning_rate": 4.722222222222222e-05,
      "loss": 1.0014,
      "step": 200
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.3963441252708435,
      "learning_rate": 4.708333333333334e-05,
      "loss": 1.0386,
      "step": 210
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 3.540205955505371,
      "learning_rate": 4.6944444444444446e-05,
      "loss": 0.8767,
      "step": 220
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 7.137052536010742,
      "learning_rate": 4.6805555555555556e-05,
      "loss": 0.9349,
      "step": 230
    },
    {
      "epoch": 0.2,
      "grad_norm": 3.082533359527588,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.8748,
      "step": 240
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 6.162527084350586,
      "learning_rate": 4.652777777777778e-05,
      "loss": 0.8661,
      "step": 250
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.669911503791809,
      "learning_rate": 4.638888888888889e-05,
      "loss": 0.8695,
      "step": 260
    },
    {
      "epoch": 0.225,
      "grad_norm": 5.478029251098633,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.9638,
      "step": 270
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 6.622207164764404,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 0.7955,
      "step": 280
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 5.834073066711426,
      "learning_rate": 4.5972222222222225e-05,
      "loss": 0.9605,
      "step": 290
    },
    {
      "epoch": 0.25,
      "grad_norm": 20.573877334594727,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.8622,
      "step": 300
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 4.629955768585205,
      "learning_rate": 4.569444444444444e-05,
      "loss": 0.7935,
      "step": 310
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.7197179794311523,
      "learning_rate": 4.555555555555556e-05,
      "loss": 0.8249,
      "step": 320
    },
    {
      "epoch": 0.275,
      "grad_norm": 8.972798347473145,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.7833,
      "step": 330
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 13.259066581726074,
      "learning_rate": 4.527777777777778e-05,
      "loss": 0.8342,
      "step": 340
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.41721540689468384,
      "learning_rate": 4.5138888888888894e-05,
      "loss": 1.0378,
      "step": 350
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.0462579727172852,
      "learning_rate": 4.5e-05,
      "loss": 1.0389,
      "step": 360
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 5.982383728027344,
      "learning_rate": 4.486111111111111e-05,
      "loss": 0.986,
      "step": 370
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.9095277190208435,
      "learning_rate": 4.472222222222223e-05,
      "loss": 0.8472,
      "step": 380
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.604290246963501,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.8407,
      "step": 390
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 7.583712100982666,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.9148,
      "step": 400
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 14.43315315246582,
      "learning_rate": 4.4305555555555556e-05,
      "loss": 0.9221,
      "step": 410
    },
    {
      "epoch": 0.35,
      "grad_norm": 22.541250228881836,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.8316,
      "step": 420
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 2.9860212802886963,
      "learning_rate": 4.402777777777778e-05,
      "loss": 0.899,
      "step": 430
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 3.3890554904937744,
      "learning_rate": 4.388888888888889e-05,
      "loss": 0.7582,
      "step": 440
    },
    {
      "epoch": 0.375,
      "grad_norm": 19.3521671295166,
      "learning_rate": 4.375e-05,
      "loss": 0.7996,
      "step": 450
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 18.567459106445312,
      "learning_rate": 4.3611111111111116e-05,
      "loss": 0.8829,
      "step": 460
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 121.215576171875,
      "learning_rate": 4.3472222222222225e-05,
      "loss": 0.8018,
      "step": 470
    },
    {
      "epoch": 0.4,
      "grad_norm": 45.181888580322266,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.8742,
      "step": 480
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 3.265338659286499,
      "learning_rate": 4.319444444444445e-05,
      "loss": 0.7504,
      "step": 490
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 7.685157299041748,
      "learning_rate": 4.305555555555556e-05,
      "loss": 0.9947,
      "step": 500
    },
    {
      "epoch": 0.425,
      "grad_norm": 2.699906587600708,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.7306,
      "step": 510
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 4.095451831817627,
      "learning_rate": 4.277777777777778e-05,
      "loss": 0.923,
      "step": 520
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 35.09462356567383,
      "learning_rate": 4.263888888888889e-05,
      "loss": 0.7633,
      "step": 530
    },
    {
      "epoch": 0.45,
      "grad_norm": 7.3118367195129395,
      "learning_rate": 4.25e-05,
      "loss": 0.8067,
      "step": 540
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.9244943857192993,
      "learning_rate": 4.236111111111111e-05,
      "loss": 0.7421,
      "step": 550
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 6.820999622344971,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.6522,
      "step": 560
    },
    {
      "epoch": 0.475,
      "grad_norm": 1.4559712409973145,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.8598,
      "step": 570
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 5.312601089477539,
      "learning_rate": 4.194444444444445e-05,
      "loss": 0.794,
      "step": 580
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 7.4759674072265625,
      "learning_rate": 4.1805555555555556e-05,
      "loss": 0.8056,
      "step": 590
    },
    {
      "epoch": 0.5,
      "grad_norm": 15.515321731567383,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.918,
      "step": 600
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 1.8445725440979004,
      "learning_rate": 4.152777777777778e-05,
      "loss": 0.7271,
      "step": 610
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 5.529647350311279,
      "learning_rate": 4.138888888888889e-05,
      "loss": 0.7361,
      "step": 620
    },
    {
      "epoch": 0.525,
      "grad_norm": 1.0003446340560913,
      "learning_rate": 4.125e-05,
      "loss": 0.7318,
      "step": 630
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 6.974701404571533,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.823,
      "step": 640
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 24.991491317749023,
      "learning_rate": 4.0972222222222225e-05,
      "loss": 0.6627,
      "step": 650
    },
    {
      "epoch": 0.55,
      "grad_norm": 16.407896041870117,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.7378,
      "step": 660
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 13.625863075256348,
      "learning_rate": 4.0694444444444444e-05,
      "loss": 0.8225,
      "step": 670
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 1.2966413497924805,
      "learning_rate": 4.055555555555556e-05,
      "loss": 0.826,
      "step": 680
    },
    {
      "epoch": 0.575,
      "grad_norm": 6.591843605041504,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.9946,
      "step": 690
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 52.9169921875,
      "learning_rate": 4.027777777777778e-05,
      "loss": 0.9158,
      "step": 700
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 1.3059498071670532,
      "learning_rate": 4.0138888888888894e-05,
      "loss": 0.8967,
      "step": 710
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.884739398956299,
      "learning_rate": 4e-05,
      "loss": 0.9212,
      "step": 720
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 8.441914558410645,
      "learning_rate": 3.986111111111111e-05,
      "loss": 1.0656,
      "step": 730
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 5.622957706451416,
      "learning_rate": 3.972222222222222e-05,
      "loss": 0.8186,
      "step": 740
    },
    {
      "epoch": 0.625,
      "grad_norm": 16.94982147216797,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.8922,
      "step": 750
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 2.1309120655059814,
      "learning_rate": 3.944444444444445e-05,
      "loss": 0.8155,
      "step": 760
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.7266406416893005,
      "learning_rate": 3.9305555555555556e-05,
      "loss": 0.7371,
      "step": 770
    },
    {
      "epoch": 0.65,
      "grad_norm": 27.197912216186523,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.8795,
      "step": 780
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 2.3473966121673584,
      "learning_rate": 3.902777777777778e-05,
      "loss": 0.8998,
      "step": 790
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 6.85922908782959,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.9191,
      "step": 800
    },
    {
      "epoch": 0.675,
      "grad_norm": 10.86136531829834,
      "learning_rate": 3.875e-05,
      "loss": 0.8121,
      "step": 810
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 1.761768102645874,
      "learning_rate": 3.8611111111111116e-05,
      "loss": 0.9496,
      "step": 820
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 4.40264368057251,
      "learning_rate": 3.8472222222222225e-05,
      "loss": 0.8072,
      "step": 830
    },
    {
      "epoch": 0.7,
      "grad_norm": 10.142815589904785,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.7445,
      "step": 840
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 19.180606842041016,
      "learning_rate": 3.8194444444444444e-05,
      "loss": 0.7341,
      "step": 850
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 1.6519628763198853,
      "learning_rate": 3.805555555555555e-05,
      "loss": 0.6799,
      "step": 860
    },
    {
      "epoch": 0.725,
      "grad_norm": 7.562361240386963,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.7479,
      "step": 870
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 9.997169494628906,
      "learning_rate": 3.777777777777778e-05,
      "loss": 0.7893,
      "step": 880
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 13.043940544128418,
      "learning_rate": 3.763888888888889e-05,
      "loss": 0.709,
      "step": 890
    },
    {
      "epoch": 0.75,
      "grad_norm": 351.52899169921875,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.8054,
      "step": 900
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 42.70247268676758,
      "learning_rate": 3.736111111111111e-05,
      "loss": 0.8794,
      "step": 910
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 65.21479797363281,
      "learning_rate": 3.722222222222222e-05,
      "loss": 0.8731,
      "step": 920
    },
    {
      "epoch": 0.775,
      "grad_norm": 29.305051803588867,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.7061,
      "step": 930
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 5.341578006744385,
      "learning_rate": 3.694444444444445e-05,
      "loss": 0.8039,
      "step": 940
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 47.97091293334961,
      "learning_rate": 3.6805555555555556e-05,
      "loss": 0.6278,
      "step": 950
    },
    {
      "epoch": 0.8,
      "grad_norm": 32.317806243896484,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.683,
      "step": 960
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 7.821030139923096,
      "learning_rate": 3.6527777777777775e-05,
      "loss": 0.7723,
      "step": 970
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 15.797428131103516,
      "learning_rate": 3.638888888888889e-05,
      "loss": 0.7863,
      "step": 980
    },
    {
      "epoch": 0.825,
      "grad_norm": 4.439980506896973,
      "learning_rate": 3.625e-05,
      "loss": 0.5444,
      "step": 990
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 2.408932685852051,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.8489,
      "step": 1000
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 35.31509017944336,
      "learning_rate": 3.5972222222222225e-05,
      "loss": 0.9974,
      "step": 1010
    },
    {
      "epoch": 0.85,
      "grad_norm": 2.595299243927002,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.874,
      "step": 1020
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 3.294362783432007,
      "learning_rate": 3.5694444444444444e-05,
      "loss": 0.7007,
      "step": 1030
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 16.64017677307129,
      "learning_rate": 3.555555555555556e-05,
      "loss": 0.8756,
      "step": 1040
    },
    {
      "epoch": 0.875,
      "grad_norm": 11.324993133544922,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.7194,
      "step": 1050
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 11.919088363647461,
      "learning_rate": 3.527777777777778e-05,
      "loss": 0.8368,
      "step": 1060
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 3.5452895164489746,
      "learning_rate": 3.513888888888889e-05,
      "loss": 0.7195,
      "step": 1070
    },
    {
      "epoch": 0.9,
      "grad_norm": 6.323110103607178,
      "learning_rate": 3.5e-05,
      "loss": 0.6794,
      "step": 1080
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 25.8367919921875,
      "learning_rate": 3.486111111111111e-05,
      "loss": 0.6389,
      "step": 1090
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 24.070098876953125,
      "learning_rate": 3.472222222222222e-05,
      "loss": 0.7585,
      "step": 1100
    },
    {
      "epoch": 0.925,
      "grad_norm": 41.01952362060547,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.6726,
      "step": 1110
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 6.331752300262451,
      "learning_rate": 3.444444444444445e-05,
      "loss": 0.7503,
      "step": 1120
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 39.63605880737305,
      "learning_rate": 3.430555555555556e-05,
      "loss": 0.8225,
      "step": 1130
    },
    {
      "epoch": 0.95,
      "grad_norm": 15.437686920166016,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.8387,
      "step": 1140
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 95.84567260742188,
      "learning_rate": 3.402777777777778e-05,
      "loss": 0.8116,
      "step": 1150
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 7.996632099151611,
      "learning_rate": 3.388888888888889e-05,
      "loss": 0.7831,
      "step": 1160
    },
    {
      "epoch": 0.975,
      "grad_norm": 13.071514129638672,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.7189,
      "step": 1170
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 24.43753433227539,
      "learning_rate": 3.3611111111111116e-05,
      "loss": 0.8267,
      "step": 1180
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 3.346543788909912,
      "learning_rate": 3.347222222222222e-05,
      "loss": 0.8305,
      "step": 1190
    },
    {
      "epoch": 1.0,
      "grad_norm": 10.888394355773926,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.6755,
      "step": 1200
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7333333333333333,
      "eval_f1": 0.7290343654124136,
      "eval_loss": 0.7386016845703125,
      "eval_runtime": 230.2557,
      "eval_samples_per_second": 10.423,
      "eval_steps_per_second": 1.303,
      "step": 1200
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 13.115168571472168,
      "learning_rate": 3.3194444444444444e-05,
      "loss": 0.7935,
      "step": 1210
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 4.9654741287231445,
      "learning_rate": 3.3055555555555553e-05,
      "loss": 0.5458,
      "step": 1220
    },
    {
      "epoch": 1.025,
      "grad_norm": 3.6337602138519287,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.6549,
      "step": 1230
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.536027193069458,
      "learning_rate": 3.277777777777778e-05,
      "loss": 0.7188,
      "step": 1240
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 41.92660903930664,
      "learning_rate": 3.263888888888889e-05,
      "loss": 0.4531,
      "step": 1250
    },
    {
      "epoch": 1.05,
      "grad_norm": 7.9756855964660645,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.7115,
      "step": 1260
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 4.8796162605285645,
      "learning_rate": 3.236111111111111e-05,
      "loss": 0.7787,
      "step": 1270
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 34.346160888671875,
      "learning_rate": 3.222222222222223e-05,
      "loss": 0.7878,
      "step": 1280
    },
    {
      "epoch": 1.075,
      "grad_norm": 20.577791213989258,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.7454,
      "step": 1290
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 16.594547271728516,
      "learning_rate": 3.194444444444444e-05,
      "loss": 0.6358,
      "step": 1300
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 8.638367652893066,
      "learning_rate": 3.180555555555556e-05,
      "loss": 0.6954,
      "step": 1310
    },
    {
      "epoch": 1.1,
      "grad_norm": 4.474763870239258,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.6013,
      "step": 1320
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.9338061809539795,
      "learning_rate": 3.1527777777777775e-05,
      "loss": 0.6858,
      "step": 1330
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.9991164207458496,
      "learning_rate": 3.138888888888889e-05,
      "loss": 0.8324,
      "step": 1340
    },
    {
      "epoch": 1.125,
      "grad_norm": 35.75359344482422,
      "learning_rate": 3.125e-05,
      "loss": 0.6767,
      "step": 1350
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 16.138158798217773,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.7473,
      "step": 1360
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 22.13149070739746,
      "learning_rate": 3.0972222222222226e-05,
      "loss": 0.8308,
      "step": 1370
    },
    {
      "epoch": 1.15,
      "grad_norm": 3.0993924140930176,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.6698,
      "step": 1380
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 44.435035705566406,
      "learning_rate": 3.069444444444445e-05,
      "loss": 0.7765,
      "step": 1390
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 8.568260192871094,
      "learning_rate": 3.055555555555556e-05,
      "loss": 0.7588,
      "step": 1400
    },
    {
      "epoch": 1.175,
      "grad_norm": 23.65774154663086,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.5992,
      "step": 1410
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.9786040186882019,
      "learning_rate": 3.0277777777777776e-05,
      "loss": 0.7275,
      "step": 1420
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 6.047706127166748,
      "learning_rate": 3.0138888888888888e-05,
      "loss": 0.747,
      "step": 1430
    },
    {
      "epoch": 1.2,
      "grad_norm": 10.537473678588867,
      "learning_rate": 3e-05,
      "loss": 0.6702,
      "step": 1440
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 19.293949127197266,
      "learning_rate": 2.9861111111111113e-05,
      "loss": 0.7322,
      "step": 1450
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 4.060578346252441,
      "learning_rate": 2.9722222222222223e-05,
      "loss": 0.5791,
      "step": 1460
    },
    {
      "epoch": 1.225,
      "grad_norm": 24.609872817993164,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.5999,
      "step": 1470
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 151.47613525390625,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 0.5852,
      "step": 1480
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 12.648322105407715,
      "learning_rate": 2.9305555555555557e-05,
      "loss": 0.6538,
      "step": 1490
    },
    {
      "epoch": 1.25,
      "grad_norm": 57.76516342163086,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.7189,
      "step": 1500
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 9.997895240783691,
      "learning_rate": 2.9027777777777782e-05,
      "loss": 0.7585,
      "step": 1510
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 20.66631317138672,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 0.7382,
      "step": 1520
    },
    {
      "epoch": 1.275,
      "grad_norm": 81.81233215332031,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.6413,
      "step": 1530
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 14.947548866271973,
      "learning_rate": 2.861111111111111e-05,
      "loss": 0.562,
      "step": 1540
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 10.313830375671387,
      "learning_rate": 2.8472222222222223e-05,
      "loss": 0.679,
      "step": 1550
    },
    {
      "epoch": 1.3,
      "grad_norm": 121.72452545166016,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.6671,
      "step": 1560
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 1.9061506986618042,
      "learning_rate": 2.8194444444444445e-05,
      "loss": 0.5619,
      "step": 1570
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 21.050025939941406,
      "learning_rate": 2.8055555555555557e-05,
      "loss": 0.6924,
      "step": 1580
    },
    {
      "epoch": 1.325,
      "grad_norm": 2.1272225379943848,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.6655,
      "step": 1590
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 32.7474479675293,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.6153,
      "step": 1600
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 14.47484302520752,
      "learning_rate": 2.7638888888888892e-05,
      "loss": 0.5571,
      "step": 1610
    },
    {
      "epoch": 1.35,
      "grad_norm": 36.53000259399414,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.7963,
      "step": 1620
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 89.21116638183594,
      "learning_rate": 2.7361111111111114e-05,
      "loss": 0.7305,
      "step": 1630
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 1.3442310094833374,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 0.5905,
      "step": 1640
    },
    {
      "epoch": 1.375,
      "grad_norm": 3.158205032348633,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.5934,
      "step": 1650
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 33.96196365356445,
      "learning_rate": 2.6944444444444445e-05,
      "loss": 0.5544,
      "step": 1660
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 12.220074653625488,
      "learning_rate": 2.6805555555555557e-05,
      "loss": 1.0132,
      "step": 1670
    },
    {
      "epoch": 1.4,
      "grad_norm": 6.545083999633789,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.5479,
      "step": 1680
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 1.8308720588684082,
      "learning_rate": 2.652777777777778e-05,
      "loss": 0.6039,
      "step": 1690
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 2.9439573287963867,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 0.6966,
      "step": 1700
    },
    {
      "epoch": 1.425,
      "grad_norm": 46.84357833862305,
      "learning_rate": 2.625e-05,
      "loss": 0.6431,
      "step": 1710
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 1.2637358903884888,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 0.5844,
      "step": 1720
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 28.3543701171875,
      "learning_rate": 2.5972222222222226e-05,
      "loss": 0.7739,
      "step": 1730
    },
    {
      "epoch": 1.45,
      "grad_norm": 12.510887145996094,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.6006,
      "step": 1740
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 1.374215841293335,
      "learning_rate": 2.5694444444444445e-05,
      "loss": 0.7572,
      "step": 1750
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 12.121230125427246,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 0.5853,
      "step": 1760
    },
    {
      "epoch": 1.475,
      "grad_norm": 12.443790435791016,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.6381,
      "step": 1770
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 24.51559829711914,
      "learning_rate": 2.527777777777778e-05,
      "loss": 0.6275,
      "step": 1780
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 5.455292701721191,
      "learning_rate": 2.513888888888889e-05,
      "loss": 0.7756,
      "step": 1790
    },
    {
      "epoch": 1.5,
      "grad_norm": 18.313640594482422,
      "learning_rate": 2.5e-05,
      "loss": 0.6979,
      "step": 1800
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 5.97682523727417,
      "learning_rate": 2.4861111111111114e-05,
      "loss": 0.551,
      "step": 1810
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 11.95600700378418,
      "learning_rate": 2.4722222222222223e-05,
      "loss": 0.4277,
      "step": 1820
    },
    {
      "epoch": 1.525,
      "grad_norm": 22.924747467041016,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.5786,
      "step": 1830
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 31.441009521484375,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.6839,
      "step": 1840
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 3.025400161743164,
      "learning_rate": 2.4305555555555558e-05,
      "loss": 0.7248,
      "step": 1850
    },
    {
      "epoch": 1.55,
      "grad_norm": 39.70759963989258,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.5236,
      "step": 1860
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 19.495494842529297,
      "learning_rate": 2.402777777777778e-05,
      "loss": 0.5942,
      "step": 1870
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 40.081298828125,
      "learning_rate": 2.3888888888888892e-05,
      "loss": 0.6734,
      "step": 1880
    },
    {
      "epoch": 1.575,
      "grad_norm": 116.84693145751953,
      "learning_rate": 2.375e-05,
      "loss": 0.5535,
      "step": 1890
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 60.45024490356445,
      "learning_rate": 2.361111111111111e-05,
      "loss": 0.7006,
      "step": 1900
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 6.912592887878418,
      "learning_rate": 2.3472222222222223e-05,
      "loss": 0.6482,
      "step": 1910
    },
    {
      "epoch": 1.6,
      "grad_norm": 18.9747371673584,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.396,
      "step": 1920
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 18.335853576660156,
      "learning_rate": 2.3194444444444445e-05,
      "loss": 0.5821,
      "step": 1930
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 29.090620040893555,
      "learning_rate": 2.3055555555555558e-05,
      "loss": 0.6076,
      "step": 1940
    },
    {
      "epoch": 1.625,
      "grad_norm": 22.663190841674805,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.6499,
      "step": 1950
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 33.501224517822266,
      "learning_rate": 2.277777777777778e-05,
      "loss": 0.8301,
      "step": 1960
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 58.57075500488281,
      "learning_rate": 2.263888888888889e-05,
      "loss": 0.6661,
      "step": 1970
    },
    {
      "epoch": 1.65,
      "grad_norm": 39.244590759277344,
      "learning_rate": 2.25e-05,
      "loss": 0.564,
      "step": 1980
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 11.50232982635498,
      "learning_rate": 2.2361111111111114e-05,
      "loss": 0.6562,
      "step": 1990
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 9.654740333557129,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.5598,
      "step": 2000
    },
    {
      "epoch": 1.675,
      "grad_norm": 247.88433837890625,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.5047,
      "step": 2010
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 9.152338027954102,
      "learning_rate": 2.1944444444444445e-05,
      "loss": 0.4127,
      "step": 2020
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 46.66478729248047,
      "learning_rate": 2.1805555555555558e-05,
      "loss": 0.5433,
      "step": 2030
    },
    {
      "epoch": 1.7,
      "grad_norm": 27.020084381103516,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.7086,
      "step": 2040
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 28.09702491760254,
      "learning_rate": 2.152777777777778e-05,
      "loss": 0.4056,
      "step": 2050
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 10.099952697753906,
      "learning_rate": 2.138888888888889e-05,
      "loss": 0.5292,
      "step": 2060
    },
    {
      "epoch": 1.725,
      "grad_norm": 10.082840919494629,
      "learning_rate": 2.125e-05,
      "loss": 0.685,
      "step": 2070
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 36.9957389831543,
      "learning_rate": 2.111111111111111e-05,
      "loss": 0.5116,
      "step": 2080
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 6.1987690925598145,
      "learning_rate": 2.0972222222222223e-05,
      "loss": 0.5724,
      "step": 2090
    },
    {
      "epoch": 1.75,
      "grad_norm": 34.872154235839844,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.7051,
      "step": 2100
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 19.58416175842285,
      "learning_rate": 2.0694444444444445e-05,
      "loss": 0.4681,
      "step": 2110
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 42.11953353881836,
      "learning_rate": 2.0555555555555555e-05,
      "loss": 0.6889,
      "step": 2120
    },
    {
      "epoch": 1.775,
      "grad_norm": 45.301883697509766,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.7343,
      "step": 2130
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 10.647772789001465,
      "learning_rate": 2.027777777777778e-05,
      "loss": 0.7342,
      "step": 2140
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 7.029067516326904,
      "learning_rate": 2.013888888888889e-05,
      "loss": 0.6262,
      "step": 2150
    },
    {
      "epoch": 1.8,
      "grad_norm": 6.17195987701416,
      "learning_rate": 2e-05,
      "loss": 0.5828,
      "step": 2160
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 9.89739990234375,
      "learning_rate": 1.986111111111111e-05,
      "loss": 0.579,
      "step": 2170
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 95.15031433105469,
      "learning_rate": 1.9722222222222224e-05,
      "loss": 0.5836,
      "step": 2180
    },
    {
      "epoch": 1.825,
      "grad_norm": 26.089197158813477,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.7296,
      "step": 2190
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 24.53022003173828,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 0.5146,
      "step": 2200
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 8.885104179382324,
      "learning_rate": 1.9305555555555558e-05,
      "loss": 0.5576,
      "step": 2210
    },
    {
      "epoch": 1.85,
      "grad_norm": 7.2093095779418945,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.7224,
      "step": 2220
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 17.581483840942383,
      "learning_rate": 1.9027777777777776e-05,
      "loss": 0.6726,
      "step": 2230
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.5306687355041504,
      "learning_rate": 1.888888888888889e-05,
      "loss": 0.6933,
      "step": 2240
    },
    {
      "epoch": 1.875,
      "grad_norm": 59.135231018066406,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.6113,
      "step": 2250
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 15.745513916015625,
      "learning_rate": 1.861111111111111e-05,
      "loss": 0.5632,
      "step": 2260
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 27.316322326660156,
      "learning_rate": 1.8472222222222224e-05,
      "loss": 0.6099,
      "step": 2270
    },
    {
      "epoch": 1.9,
      "grad_norm": 29.781795501708984,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.6378,
      "step": 2280
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 11.53860855102539,
      "learning_rate": 1.8194444444444445e-05,
      "loss": 0.813,
      "step": 2290
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 52.32182693481445,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 0.7043,
      "step": 2300
    },
    {
      "epoch": 1.925,
      "grad_norm": 13.253856658935547,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.4613,
      "step": 2310
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 49.281585693359375,
      "learning_rate": 1.777777777777778e-05,
      "loss": 0.58,
      "step": 2320
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 18.98868751525879,
      "learning_rate": 1.763888888888889e-05,
      "loss": 0.7619,
      "step": 2330
    },
    {
      "epoch": 1.95,
      "grad_norm": 11.305363655090332,
      "learning_rate": 1.75e-05,
      "loss": 0.438,
      "step": 2340
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 45.8193244934082,
      "learning_rate": 1.736111111111111e-05,
      "loss": 0.6572,
      "step": 2350
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 6.891228199005127,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 0.505,
      "step": 2360
    },
    {
      "epoch": 1.975,
      "grad_norm": 18.97136116027832,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.6549,
      "step": 2370
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.6755759119987488,
      "learning_rate": 1.6944444444444446e-05,
      "loss": 0.5661,
      "step": 2380
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 14.514326095581055,
      "learning_rate": 1.6805555555555558e-05,
      "loss": 0.4945,
      "step": 2390
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.1287343502044678,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.6011,
      "step": 2400
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.7729166666666667,
      "eval_f1": 0.7733328792852531,
      "eval_loss": 0.6374692916870117,
      "eval_runtime": 227.8809,
      "eval_samples_per_second": 10.532,
      "eval_steps_per_second": 1.316,
      "step": 2400
    }
  ],
  "logging_steps": 10,
  "max_steps": 3600,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1262944405094400.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
